ðŸ§  Multi-Model Classification Playground
Welcome to the Multi-Model Classification Playground!
In this project, I've trained and compared multiple classification algorithms on a dataset to understand their performance differences, strengths, and weaknesses. The final results are shared and analyzed at the end â€” including accuracy, precision, recall, F1 scores, and a bonus comparison based on real-world implications.

The following classification algorithms were trained and evaluated:

Logistic Regression

Decision Tree

Random Forest

K-Nearest Neighbors (KNN)

XGBoost (bonus/high-performance)

AdaBoost


ðŸ“Š Evaluation Metrics
Each model was evaluated using the following metrics, when used train_test_split:

Accuracy

Precision

Recall

F1 Score

Confusion Matrix

All metrics were computed using stratified train-test split and then applied my leanings of k-fold cross-validation for robust performance estimation.


ðŸŒŸ Bonus Assignment
ðŸ“Œ Refer to the image attached for the bonus assignment.

ðŸ“¥ Bonus Image Attached: KMeans-Algo-GradientDescentUpdate.jpg

